{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8048675,"sourceType":"datasetVersion","datasetId":4746267},{"sourceId":8051064,"sourceType":"datasetVersion","datasetId":4747895},{"sourceId":26688,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":22467}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Dependencies","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"!pip install accelerate==0.21.0 \\\n  bitsandbytes==0.40.2 \\\n  peft==0.5.0 \\\n  transformers==4.34.0 \\\n  sentencepiece","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:42:58.881455Z","iopub.execute_input":"2024-04-07T12:42:58.881812Z","iopub.status.idle":"2024-04-07T12:43:29.760969Z","shell.execute_reply.started":"2024-04-07T12:42:58.881782Z","shell.execute_reply":"2024-04-07T12:43:29.759827Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting accelerate==0.21.0\n  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\nCollecting bitsandbytes==0.40.2\n  Downloading bitsandbytes-0.40.2-py3-none-any.whl.metadata (9.8 kB)\nCollecting peft==0.5.0\n  Downloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\nCollecting transformers==4.34.0\n  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.21.0) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (4.66.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft==0.5.0) (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (0.21.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.34.0) (2.31.0)\nCollecting tokenizers<0.15,>=0.14 (from transformers==4.34.0)\n  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (2024.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.21.0) (3.1.1)\nCollecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0)\n  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.34.0) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\nDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.40.2-py3-none-any.whl (92.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.5.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, huggingface-hub, tokenizers, accelerate, transformers, peft\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.21.4\n    Uninstalling huggingface-hub-0.21.4:\n      Successfully uninstalled huggingface-hub-0.21.4\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.28.0\n    Uninstalling accelerate-0.28.0:\n      Successfully uninstalled accelerate-0.28.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.2\n    Uninstalling transformers-4.38.2:\n      Successfully uninstalled transformers-4.38.2\nSuccessfully installed accelerate-0.21.0 bitsandbytes-0.40.2 huggingface-hub-0.17.3 peft-0.5.0 tokenizers-0.14.1 transformers-4.34.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:43:29.762898Z","iopub.execute_input":"2024-04-07T12:43:29.763202Z","iopub.status.idle":"2024-04-07T12:43:42.342395Z","shell.execute_reply.started":"2024-04-07T12:43:29.763173Z","shell.execute_reply":"2024-04-07T12:43:42.341373Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.5.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.34.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2024.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.17.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2.31.0)\nRequirement already satisfied: tokenizers<0.15,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.14.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers->peft) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install streamlit\n!npm install localtunnel","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:43:42.343912Z","iopub.execute_input":"2024-04-07T12:43:42.344318Z","iopub.status.idle":"2024-04-07T12:44:00.523472Z","shell.execute_reply.started":"2024-04-07T12:43:42.344280Z","shell.execute_reply":"2024-04-07T12:44:00.522487Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting streamlit\n  Downloading streamlit-1.33.0-py2.py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (5.2.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.7.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (4.2.4)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (8.1.7)\nRequirement already satisfied: numpy<2,>=1.19.3 in /opt/conda/lib/python3.10/site-packages (from streamlit) (1.26.4)\nRequirement already satisfied: packaging<25,>=16.8 in /opt/conda/lib/python3.10/site-packages (from streamlit) (21.3)\nRequirement already satisfied: pandas<3,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (2.1.4)\nRequirement already satisfied: pillow<11,>=7.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (9.5.0)\nRequirement already satisfied: protobuf<5,>=3.20 in /opt/conda/lib/python3.10/site-packages (from streamlit) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (11.0.0)\nRequirement already satisfied: requests<3,>=2.27 in /opt/conda/lib/python3.10/site-packages (from streamlit) (2.31.0)\nRequirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (13.7.0)\nRequirement already satisfied: tenacity<9,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (8.2.3)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit) (0.10.2)\nRequirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from streamlit) (4.9.0)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit) (3.1.41)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit) (6.3.3)\nCollecting watchdog>=2.1.5 (from streamlit)\n  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (4.20.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit) (0.12.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25,>=16.8->streamlit) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.16.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\nDownloading streamlit-1.33.0-py2.py3-none-any.whl (8.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\nSuccessfully installed pydeck-0.8.1b0 streamlit-1.33.0 watchdog-4.0.0\n\u001b[K\u001b[?25hm#################\u001b[0m\u001b[100;90m.\u001b[0m] - reify:debug: \u001b[32;40mhttp\u001b[0m \u001b[35mfetch\u001b[0m GET 200 https://registry.npmjs.o\u001b[0m\u001b[K\nadded 22 packages in 2s\n\n3 packages are looking for funding\n  run `npm fund` for details\n\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m New \u001b[33mminor\u001b[39m version of npm available! \u001b[31m10.1.0\u001b[39m -> \u001b[32m10.5.1\u001b[39m\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Changelog: \u001b[36mhttps://github.com/npm/cli/releases/tag/v10.5.1\u001b[39m\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m Run \u001b[32mnpm install -g npm@10.5.1\u001b[39m to update!\n\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[36;40mnotice\u001b[0m\u001b[35m\u001b[0m \n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Reader","metadata":{}},{"cell_type":"code","source":"%%writefile data_reader.py\n\nimport pandas as pd\n\n\ndf_ste = pd.read_excel(\"/kaggle/input/tenderhack-msk/TenderHack_msk.xlsx\", sheet_name=\"СТЕ\")\ndf_char = pd.read_excel(\"/kaggle/input/tenderhack-msk/TenderHack_msk.xlsx\", sheet_name=\"Характеристики\")\n\ndf_ste[\"Название СТЕ\"] = df_ste[\"Название СТЕ\"].str.lower()\ndf_ste[\"Наименование конечной категории Портала\"] = df_ste[\"Наименование конечной категории Портала\"].str.lower()\n\nunique_targets = df_ste[\"Наименование конечной категории Портала\"].unique()\ntarget2id = {target: idx for idx, target in enumerate(unique_targets)}\nid2target = {idx: target for idx, target in enumerate(unique_targets)}\n\ntest_top = pd.read_csv(\"/kaggle/input/top-15-characteristic-for-category/top_15_characteristic_for_category.csv\")\ntest_top[\"Наименование конечной категории Портала\"] = test_top[\"Наименование конечной категории Портала\"].str.lower()\ncategory_dict = test_top.groupby('Наименование конечной категории Портала')['Название характеристики'].apply(list).to_dict()","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:44:00.526054Z","iopub.execute_input":"2024-04-07T12:44:00.526396Z","iopub.status.idle":"2024-04-07T12:44:00.535006Z","shell.execute_reply.started":"2024-04-07T12:44:00.526364Z","shell.execute_reply":"2024-04-07T12:44:00.533943Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Writing data_reader.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Ozon Parser","metadata":{}},{"cell_type":"code","source":"%%writefile ozon_parser.py\n\nimport re\nfrom urllib.parse import quote\nfrom urllib.request import urlopen, Request\nfrom bs4 import BeautifulSoup\n\n\ndef get_encode_url_from_source_text(text: str) -> str:\n    \"\"\" Кодирование запроса в ссылке \"\"\"\n    encoded_text = quote(text, safe='')\n    url = f\"https://www.ozon.ru/search/?text={encoded_text}&from_global=true\"\n    return url\n\n\ndef get_soup_from_url(url: str):\n    r = Request(url)\n    html = urlopen(r).read()\n    soup = BeautifulSoup(html, features=\"html.parser\")\n    return soup\n\n\ndef get_all_product_from_url(soup) -> list[str]:\n    \"\"\" Получить все ссылки на товары на странице запроса \"\"\"\n    product_urls = []\n    for link in soup.find_all('a'):\n        href = link.get('href')\n        if href and '/product/' in href:\n            product_urls.append(href)\n    return product_urls\n\n\ndef get_text_from_url(url: str) -> str:\n    \"\"\" Получение текста со страницы \"\"\"\n    soup = get_soup_from_url(url)\n    for script in soup([\"script\", \"style\"]):\n        script.extract()\n    text = soup.get_text()\n    lines = (line.strip() for line in text.splitlines())\n    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n    text = '\\n'.join(chunk for chunk in chunks if chunk)\n    return text\n\n\ndef clean_string(text, trash_words):\n    \"\"\" Чистка текста [Deprecated] \"\"\"\n    text = text.split(\"\\nОтзывы о товаре\", 1)[0]\n    text = text.split(\"\\nХарактеристики\", 1)[1]\n    for word in trash_words:\n        text = re.sub(r'\\n' + word + r'.*?(?=\\n|$)', '', text)\n    return text\n\n\ndef get_clean_from_roma(url):\n    \"\"\" Чистка текста \"\"\"\n    text = get_text_from_url(url)\n    text = text.split(\"Отзывы о товаре\", 1)[0]\n    trimmed_string = text.split(\"Подборки товаров\")[0]\n    if trimmed_string.endswith(\" \"):\n        trimmed_string = trimmed_string[:-1]\n    text = trimmed_string.split(\"Характеристики\")[1]\n    return text\n\n\ndef get_products_urls_from_query(query):\n    \"\"\" Получение ссылок на товары в маркетплейсе Ozon \"\"\"\n    OZON_SRC_URL = \"https://www.ozon.ru\"\n    encoded_query = get_encode_url_from_source_text(query)\n    soup = get_soup_from_url(encoded_query)\n    urls = [OZON_SRC_URL + url for url in get_all_product_from_url(soup)]\n    return urls\n\n\ndef get_characteristics_from_query(query):\n    urls = get_products_urls_from_query(query)\n    if len(urls) == 0:\n        return None\n    else:\n        text_characteristics = get_clean_from_roma(urls[0])\n    return text_characteristics\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:44:00.536455Z","iopub.execute_input":"2024-04-07T12:44:00.536781Z","iopub.status.idle":"2024-04-07T12:44:00.552334Z","shell.execute_reply.started":"2024-04-07T12:44:00.536755Z","shell.execute_reply":"2024-04-07T12:44:00.551539Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Writing ozon_parser.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## cointegrated/rubert-tiny","metadata":{}},{"cell_type":"code","source":"%%writefile category_model.py\n\nfrom data_reader import target2id, id2target\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AdamW\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, texts, targets, target2id, tokenizer, max_len=512):\n        self.texts = texts\n        self.targets = targets\n        self.target2id = target2id\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        target = self.target2id[self.targets[idx]]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'targets': torch.tensor(target, dtype=torch.long)\n        }\n\n\n\nclass BertClassifier:\n    def __init__(self, model_path, tokenizer_path, n_classes, epochs, model_save_path='bert.pt'):\n        self.model = BertForSequenceClassification.from_pretrained(model_path)\n        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.model_save_path=model_save_path\n        self.max_len = 512\n        self.epochs = epochs\n        self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n        self.model.classifier = torch.nn.Linear(self.out_features, n_classes)\n        self.model.to(self.device)\n\n    def preparation(self, X_train, y_train, X_valid, y_valid):\n        self.train_set = CustomDataset(X_train, y_train, target2id, self.tokenizer)\n        self.valid_set = CustomDataset(X_valid, y_valid, target2id, self.tokenizer)\n\n        self.train_loader = DataLoader(self.train_set, batch_size=32, shuffle=True)\n        self.valid_loader = DataLoader(self.valid_set, batch_size=32, shuffle=True)\n\n        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n        self.loss_fn = torch.nn.CrossEntropyLoss().to(self.device)\n\n    def fit(self):\n        self.model = self.model.train()\n        losses = []\n        correct_predictions = 0\n\n        for data in self.train_loader:\n            input_ids = data[\"input_ids\"].to(self.device)\n            attention_mask = data[\"attention_mask\"].to(self.device)\n            targets = data[\"targets\"].to(self.device)\n\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n                )\n\n            preds = torch.argmax(outputs.logits, dim=1)\n            loss = self.loss_fn(outputs.logits, targets)\n\n            correct_predictions += torch.sum(preds == targets)\n\n            losses.append(loss.item())\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n        train_acc = correct_predictions.double() / len(self.train_set)\n        train_loss = np.mean(losses)\n        return train_acc, train_loss\n\n    def eval(self):\n        self.model = self.model.eval()\n        losses = []\n        correct_predictions = 0\n\n        with torch.no_grad():\n            for data in self.valid_loader:\n                input_ids = data[\"input_ids\"].to(self.device)\n                attention_mask = data[\"attention_mask\"].to(self.device)\n                targets = data[\"targets\"].to(self.device)\n\n                outputs = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask\n                    )\n\n                preds = torch.argmax(outputs.logits, dim=1)\n                loss = self.loss_fn(outputs.logits, targets)\n                correct_predictions += torch.sum(preds == targets)\n                losses.append(loss.item())\n\n        val_acc = correct_predictions.double() / len(self.valid_set)\n        val_loss = np.mean(losses)\n        return val_acc, val_loss\n\n    def train(self):\n        best_accuracy = 0\n        for epoch in range(self.epochs):\n            print(f'Epoch {epoch + 1}/{self.epochs}')\n            train_acc, train_loss = self.fit()\n            print(f'Train loss {train_loss} accuracy {train_acc}')\n\n            val_acc, val_loss = self.eval()\n            print(f'Val loss {val_loss} accuracy {val_acc}')\n            print('-' * 10)\n\n            if val_acc > best_accuracy:\n                torch.save(self.model, self.model_save_path)\n                best_accuracy = val_acc\n\n        self.model = torch.load(self.model_save_path)\n\n    def predict(self, text):\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            truncation=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        out = {\n              'text': text,\n              'input_ids': encoding['input_ids'].flatten(),\n              'attention_mask': encoding['attention_mask'].flatten()\n          }\n\n        input_ids = out[\"input_ids\"].to(self.device)\n        attention_mask = out[\"attention_mask\"].to(self.device)\n\n        outputs = self.model(\n            input_ids=input_ids.unsqueeze(0),\n            attention_mask=attention_mask.unsqueeze(0)\n        )\n\n        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n        return prediction\n\n\nTOKENIZER_PATH = 'cointegrated/rubert-tiny'\nMODEL_PATH = 'cointegrated/rubert-tiny'\nMAX_EPOCHS = 10\nMODEL_SAVE_PATH = 'bert.pt'\n    \nmodel = BertClassifier(MODEL_PATH, TOKENIZER_PATH, len(target2id), MAX_EPOCHS, MODEL_SAVE_PATH)\nmodel.model = torch.load(\"/kaggle/input/rubert-classification/pytorch/v1/1/bert (3).pt\", map_location=torch.device('cuda'))","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:44:00.553444Z","iopub.execute_input":"2024-04-07T12:44:00.553750Z","iopub.status.idle":"2024-04-07T12:44:00.568551Z","shell.execute_reply.started":"2024-04-07T12:44:00.553720Z","shell.execute_reply":"2024-04-07T12:44:00.567583Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Writing category_model.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## saiga_mistral_7b_lora","metadata":{"execution":{"iopub.status.busy":"2024-04-07T02:39:35.141434Z","iopub.execute_input":"2024-04-07T02:39:35.142302Z","iopub.status.idle":"2024-04-07T02:39:35.166343Z","shell.execute_reply.started":"2024-04-07T02:39:35.142273Z","shell.execute_reply":"2024-04-07T02:39:35.165077Z"}}},{"cell_type":"code","source":"%%writefile llm_model.py\n\nimport torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n\n\nMODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\nDEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\nDEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\nDEFAULT_SYSTEM_PROMPT = \"Вы - эксперт по анализу товаров и технических спецификаций. Ваша задача - извлечь характеристики из описания товара, представленного в виде текста. Каждая характеристика имеет свое название и соответствующее значение.\"\n\n\nclass Conversation:\n    def __init__(\n        self,\n        message_template=DEFAULT_MESSAGE_TEMPLATE,\n        system_prompt=DEFAULT_SYSTEM_PROMPT,\n        response_template=DEFAULT_RESPONSE_TEMPLATE\n    ):\n        self.message_template = message_template\n        self.response_template = response_template\n        self.messages = [{\n            \"role\": \"system\",\n            \"content\": system_prompt\n        }]\n\n    def add_user_message(self, message):\n        self.messages.append({\n            \"role\": \"user\",\n            \"content\": message\n        })\n\n    def add_bot_message(self, message):\n        self.messages.append({\n            \"role\": \"bot\",\n            \"content\": message\n        })\n\n    def get_prompt(self, tokenizer):\n        final_text = \"\"\n        for message in self.messages:\n            message_text = self.message_template.format(**message)\n            final_text += message_text\n        final_text += DEFAULT_RESPONSE_TEMPLATE\n        return final_text.strip()\n\n\ndef generate(model, tokenizer, prompt, generation_config):\n    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n    data = {k: v.to(model.device) for k, v in data.items()}\n    output_ids = model.generate(\n        **data,\n        generation_config=generation_config\n    )[0]\n    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n    return output.strip()\n\n\ndef generate_ste(query, text_characteristics, model, tokenizer, generation_config, top_characteristics):\n    inputs = [\n        f\"\"\"\n        Вы решили автоматизировать процесс создания JSON-файлов с характеристиками товаров на основе предоставленных текстовых данных. \n        Вам нужно создать JSON-объект, содержащий характеристики товара, извлеченные из текста. Обязательные характеристики: наименование, категория, бренд, модель. Остальные нужно вычленить из текста.\n        Необходимо сделать результат на русском языке, кроме наименований. Входной текст: {text_characteristics}\n        \"\"\"\n#         f\"\"\"\n#         Вы решили автоматизировать процесс создания JSON-файлов с характеристиками товаров на основе предоставленных текстовых данных. \n#         Вам нужно создать JSON-объект, содержащий характеристики товара, извлеченные из текста.\n#         Обязательные характеристики: наименование, категория, бренд, модель. \n#         Также нужно добавить остальные характеристики товара из текста.\n#         Выход модели должен быть исключительно на русском языке, кроме наименований. Если обязательная характеристика не встретилась, запиши null.\n#         Входной текст: {query}. {text_characteristics}\"\"\"\n#         f\"\"\"\n#         Создайте JSON-объект с характеристиками товара на основе предоставленных данных.\n#         Обязательные характеристики: наименование, категория, бренд, модель, {top_characteristics}.\n#         Также добавь неограниченное количество дополнительных характеристик, которые найдешь во входных данных.\n#         Все характеристики должны быть на русском языке, за исключением наименования.\n#         Входные данные: {query}. {text_characteristics}\n#         \"\"\"\n#       f\"\"\"\n#       Создайте JSON-объект с характеристиками товара на основе предоставленных данных. Обязательные характеристики включают наименование, категорию, бренд и модель товара, а также {top_characteristics}. Дополнительно добавьте неограниченное количество характеристик, которые можно найти во входных данных. Все характеристики, кроме наименования, должны быть представлены на русском языке.\n#       Входные данные: {query}. {text_characteristics}\n#       Убедитесь, что вы также включаете дополнительные характеристики, если они есть, даже если они не указаны явно в обязательных характеристиках.\n#       \"\"\"\n    ]\n    outputs = []\n    for inp in inputs:\n        conversation = Conversation()\n        conversation.add_user_message(inp)\n        prompt = conversation.get_prompt(tokenizer)\n\n        output = generate(model, tokenizer, prompt, generation_config)\n        outputs.append(output)\n        print(prompt)\n        print(output)\n        print()\n        print(\"==============================\")\n        print()\n    return outputs\n\n\nconfig = PeftConfig.from_pretrained(MODEL_NAME)\nmodel = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nmodel = PeftModel.from_pretrained(\n    model,\n    MODEL_NAME,\n    torch_dtype=torch.float16\n)\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\ngeneration_config = GenerationConfig.from_pretrained(MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:44:00.569917Z","iopub.execute_input":"2024-04-07T12:44:00.570218Z","iopub.status.idle":"2024-04-07T12:44:00.584969Z","shell.execute_reply.started":"2024-04-07T12:44:00.570195Z","shell.execute_reply":"2024-04-07T12:44:00.584151Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Writing llm_model.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## StreamLit","metadata":{}},{"cell_type":"code","source":"%%writefile app.py\n\nimport json\nimport pandas as pd\nimport streamlit as st\nfrom ozon_parser import get_characteristics_from_query\n\n\ndef read_data():\n#     st.write(\"\"\"Считывание исходных данных...\"\"\")\n    from data_reader import (\n        df_ste,\n        df_char,\n        target2id,\n        id2target,\n        category_dict\n    )\n#     st.write(\"\"\"Данные считаны.\"\"\")\n    return df_ste, df_char, target2id, id2target, category_dict\n\n\ndef bert_model_init():\n#     st.write(\"\"\"Начало инициализации BERT-модели...\"\"\")\n    from category_model import (\n        model\n    )\n#     st.write(\"\"\"BERT-Модель загружена.\"\"\")\n    return model\n\n\ndef llm_model_init():\n#     st.write(\"\"\"Начало инициализации LLM-модели...\"\"\")\n    from llm_model import (\n        generate_ste,\n        model, \n        tokenizer, \n        generation_config\n    )\n#     st.write(\"\"\"LLM-Модель загружена.\"\"\")\n    return generate_ste, model, tokenizer, generation_config\n\n\ndef input_text():\n    query = st.text_input('Ввод наименования товара')\n    if query is not None:\n        characteristics = get_characteristics_from_query(query)\n        if characteristics is None:\n            st.write(\"\"\"Некорректный ввод. Попробуйте ещё раз.\"\"\")\n#     st.write(\"\"\"Результат парсинга:\"\"\")\n#     st.write(characteristics)\n        return query, characteristics\n\n\ndef get_category_and_top_characteristics(query, bert_model, id2target, category_dict):\n    category_id = bert_model.predict(query)\n    category_name = id2target[category_id]\n    top_characteristics = category_dict[category_name]\n    return category_name, top_characteristics\n\n\ndef upload():\n    df_ste, df_char, target2id, id2target, category_dict = read_data()\n    \n    bert_model = bert_model_init()\n    generate_ste, llm_model, tokenizer, generation_config = llm_model_init()\n    \n    st.title(\"ГЕНЕРАТОР ХАРАКТЕРИСТИК СТЕ\")\n    st.write(\"\"\"Веб-сайт команды GibData для генерации характеристик товаров.\"\"\")\n    \n    query, characteristics = input_text()\n    \n    category, top_characteristics = get_category_and_top_characteristics(query, bert_model, id2target, category_dict)\n#     st.write(f\"\"\"Категория: {category}\"\"\")\n#     st.write(f\"\"\"Лучшие характеристики: {top_characteristics}\"\"\")\n    \n    st.write(\"\"\"Генерация характеристик товара...\"\"\")\n    outputs = generate_ste(query, characteristics, llm_model, tokenizer, generation_config, top_characteristics)\n    st.write(\"\"\"\\nРезультат:\"\"\")\n    st.write(outputs)\n    \n    try:\n        json_data = json.loads(outputs[0])\n        df = pd.DataFrame.from_dict(json_data, orient='index').transpose()\n        edited_df = st.data_editor(df)\n    except:\n        pass\n\n\nif __name__ == \"__main__\":\n    upload()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:44:00.586287Z","iopub.execute_input":"2024-04-07T12:44:00.586548Z","iopub.status.idle":"2024-04-07T12:44:00.599608Z","shell.execute_reply.started":"2024-04-07T12:44:00.586520Z","shell.execute_reply":"2024-04-07T12:44:00.598760Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Writing app.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Running App","metadata":{}},{"cell_type":"code","source":"!curl ipv4.icanhazip.com","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:44:00.600624Z","iopub.execute_input":"2024-04-07T12:44:00.600879Z","iopub.status.idle":"2024-04-07T12:44:01.646111Z","shell.execute_reply.started":"2024-04-07T12:44:00.600858Z","shell.execute_reply":"2024-04-07T12:44:01.645073Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"35.202.163.148\n","output_type":"stream"}]},{"cell_type":"code","source":"!streamlit run app.py &>./logs.txt & npx localtunnel --port 8501","metadata":{"execution":{"iopub.status.busy":"2024-04-07T12:44:01.649332Z","iopub.execute_input":"2024-04-07T12:44:01.649671Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"your url is: https://gold-pillows-stick.loca.lt\n","output_type":"stream"}]}]}